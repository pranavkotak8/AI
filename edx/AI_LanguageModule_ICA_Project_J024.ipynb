{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_LanguageModule_ICA_Project_J024.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMP6JcoPK4hQxFFtjVDi04M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavkotak8/AI/blob/master/edx/AI_LanguageModule_ICA_Project_J024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGsgUQ655YBb"
      },
      "source": [
        "**PARSER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIwwNEcG5Z3Z"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import sys\n",
        "\n",
        "TERMINALS = \"\"\"\n",
        "Adj -> \"country\" | \"dreadful\" | \"enigmatical\" | \"little\" | \"moist\" | \"red\"\n",
        "Adv -> \"down\" | \"here\" | \"never\"\n",
        "Conj -> \"and\"\n",
        "Det -> \"a\" | \"an\" | \"his\" | \"my\" | \"the\"\n",
        "N -> \"armchair\" | \"companion\" | \"day\" | \"door\" | \"hand\" | \"he\" | \"himself\"\n",
        "N -> \"holmes\" | \"home\" | \"i\" | \"mess\" | \"paint\" | \"palm\" | \"pipe\" | \"she\"\n",
        "N -> \"smile\" | \"thursday\" | \"walk\" | \"we\" | \"word\"\n",
        "P -> \"at\" | \"before\" | \"in\" | \"of\" | \"on\" | \"to\" | \"until\"\n",
        "V -> \"arrived\" | \"came\" | \"chuckled\" | \"had\" | \"lit\" | \"said\" | \"sat\"\n",
        "V -> \"smiled\" | \"tell\" | \"were\"\n",
        "\"\"\"\n",
        "\n",
        "NONTERMINALS = \"\"\"\n",
        "S -> NP VP | S Conj S | S Conj VP\n",
        "AP -> Adj | Adj AP\n",
        "NP -> N | Det NP | AP NP | PP NP\n",
        "PP -> P NP | P S\n",
        "VP -> V | V NP | VP PP | Adv VP | VP Adv\n",
        "\"\"\"\n",
        "\n",
        "grammar = nltk.CFG.fromstring(NONTERMINALS + TERMINALS)\n",
        "parser = nltk.ChartParser(grammar)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # If filename specified, read sentence from file\n",
        "    if len(sys.argv) == 2:\n",
        "        with open(sys.argv[1]) as f:\n",
        "            s = f.read()\n",
        "\n",
        "    # Otherwise, get sentence as input\n",
        "    else:\n",
        "        s = input(\"Sentence: \")\n",
        "\n",
        "    # Convert input into list of words\n",
        "    s = preprocess(s)\n",
        "\n",
        "    # Attempt to parse sentence\n",
        "    try:\n",
        "        trees = list(parser.parse(s))\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        return\n",
        "    if not trees:\n",
        "        print(\"Could not parse sentence.\")\n",
        "        return\n",
        "\n",
        "    # Print each tree with noun phrase chunks\n",
        "    for tree in trees:\n",
        "        tree.pretty_print()\n",
        "\n",
        "        print(\"Noun Phrase Chunks\")\n",
        "        for np in np_chunk(tree):\n",
        "            print(\" \".join(np.flatten()))\n",
        "\n",
        "\n",
        "def preprocess(sentence):\n",
        "    \"\"\"\n",
        "    Convert `sentence` to a list of its words.\n",
        "    Pre-process sentence by converting all characters to lowercase\n",
        "    and removing any word that does not contain at least one alphabetic\n",
        "    character.\n",
        "    \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = word_tokenize(sentence)\n",
        "    return [word for word in words if re.match('[a-z]', word)]\n",
        "\n",
        "\n",
        "def np_chunk(tree):\n",
        "    \"\"\"\n",
        "    Return a list of all noun phrase chunks in the sentence tree.\n",
        "    A noun phrase chunk is defined as any subtree of the sentence\n",
        "    whose label is \"NP\" that does not itself contain any other\n",
        "    noun phrases as subtrees.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    for subtree in tree.subtrees(lambda t: t.label() == 'NP'):\n",
        "        if not contains_NP(subtree):\n",
        "            chunks.append(subtree)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def contains_NP(subtree):\n",
        "    for st in subtree.subtrees():\n",
        "        if st == subtree:\n",
        "            continue\n",
        "        elif st.label() == 'NP':\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL1GJfVJ5fIv"
      },
      "source": [
        "**QUESTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwVP-dCd5kly"
      },
      "source": [
        "import math\n",
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import sys\n",
        "\n",
        "FILE_MATCHES = 1\n",
        "SENTENCE_MATCHES = 1\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Check command-line arguments\n",
        "    if len(sys.argv) != 2:\n",
        "        sys.exit(\"Usage: python questions.py corpus\")\n",
        "\n",
        "    # Calculate IDF values across files\n",
        "    files = load_files(sys.argv[1])\n",
        "    file_words = {\n",
        "        filename: tokenize(files[filename])\n",
        "        for filename in files\n",
        "    }\n",
        "    file_idfs = compute_idfs(file_words)\n",
        "\n",
        "    # Prompt user for query\n",
        "    query = set(tokenize(input(\"Query: \")))\n",
        "\n",
        "    # Determine top file matches according to TF-IDF\n",
        "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
        "\n",
        "    # Extract sentences from top files\n",
        "    sentences = dict()\n",
        "    for filename in filenames:\n",
        "        for passage in files[filename].split(\"\\n\"):\n",
        "            for sentence in nltk.sent_tokenize(passage):\n",
        "                tokens = tokenize(sentence)\n",
        "                if tokens:\n",
        "                    sentences[sentence] = tokens\n",
        "\n",
        "    # Compute IDF values across sentences\n",
        "    idfs = compute_idfs(sentences)\n",
        "\n",
        "    # Determine top sentence matches\n",
        "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
        "    for match in matches:\n",
        "        print(match)\n",
        "\n",
        "\n",
        "def load_files(directory):\n",
        "    \"\"\"\n",
        "    Given a directory name, return a dictionary mapping the filename of each\n",
        "    `.txt` file inside that directory to the file's contents as a string.\n",
        "    \"\"\"\n",
        "    files_mapping = {}\n",
        "    for file_name in os.listdir(directory):\n",
        "        with open(os.path.join(directory, file_name)) as f:\n",
        "            files_mapping[file_name] = f.read()\n",
        "    return files_mapping\n",
        "\n",
        "\n",
        "def tokenize(document):\n",
        "    \"\"\"\n",
        "    Given a document (represented as a string), return a list of all of the\n",
        "    words in that document, in order.\n",
        "    Process document by coverting all words to lowercase, and removing any\n",
        "    punctuation or English stopwords.\n",
        "    \"\"\"\n",
        "    words = nltk.word_tokenize(document.lower())\n",
        "    processed_doc = []\n",
        "    for word in words:\n",
        "        if word not in nltk.corpus.stopwords.words(\"english\") and word not in string.punctuation:\n",
        "            processed_doc.append(word)\n",
        "    return processed_doc\n",
        "\n",
        "\n",
        "def compute_idfs(documents):\n",
        "    \"\"\"\n",
        "    Given a dictionary of `documents` that maps names of documents to a list\n",
        "    of words, return a dictionary that maps words to their IDF values.\n",
        "    Any word that appears in at least one of the documents should be in the\n",
        "    resulting dictionary.\n",
        "    \"\"\"\n",
        "    idfs = dict()\n",
        "    words = set()\n",
        "    total_docs = len(documents)\n",
        "    for _file in documents:\n",
        "        words.update(set(documents[_file]))\n",
        "\n",
        "    for word in words:\n",
        "        f = sum(word in documents[filename] for filename in documents)\n",
        "        idf = math.log(total_docs / f)\n",
        "        idfs[word] = idf\n",
        "    return idfs\n",
        "\n",
        "\n",
        "def top_files(query, files, idfs, n):\n",
        "    \"\"\"\n",
        "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
        "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
        "    to their IDF values), return a list of the filenames of the the `n` top\n",
        "    files that match the query, ranked according to tf-idf.\n",
        "    \"\"\"\n",
        "    tfidfs = []\n",
        "    for filename in files:\n",
        "        tfidf = 0\n",
        "        for q in query:\n",
        "            tfidf += idfs[q] * files[filename].count(q)\n",
        "        tfidfs.append((filename, tfidf))\n",
        "    tfidfs.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in tfidfs[:n]]\n",
        "\n",
        "\n",
        "def top_sentences(query, sentences, idfs, n):\n",
        "    \"\"\"\n",
        "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
        "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
        "    to their IDF values), return a list of the `n` top sentences that match\n",
        "    the query, ranked according to idf. If there are ties, preference should\n",
        "    be given to sentences that have a higher query term density.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for sentence in sentences:\n",
        "        idf = 0\n",
        "        total_words_found = 0\n",
        "        for word in query:\n",
        "            if word in sentences[sentence]:\n",
        "                total_words_found += 1\n",
        "                idf += idfs[word]\n",
        "        density = float(total_words_found) / len(sentences[sentence])\n",
        "        result.append((sentence, idf, density))\n",
        "    result.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "    return [x[0] for x in result[:n]]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}